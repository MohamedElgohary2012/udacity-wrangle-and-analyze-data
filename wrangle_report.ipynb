{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. Now we have 3 pieces of information on hand:\n",
    "- Enhanced Twitter Archive\n",
    "- Additional Data via the Twitter API\n",
    "- Image Predictions File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the course, I have two materials that can be opened directly with `pandas`\n",
    "- twitter-archive-enhanced.csv\n",
    "- image-predictions.tsv\n",
    "\n",
    "Another one is to get information through the twitter api. I used the program I wrote in `tweet.py`, got a file and saved it as a txt file, and then read it with `pandas`.\n",
    "- tweet_json.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I passed this step very quickly, and I didn't encounter too much difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part actually made me close for a long time. I used the `pandas` method `head` or `info` to browse the information at first, but the data is too much to be effectively browsed in the jupyter notebook. Later, I used Excel to open the file, but a lot of it. The information made me feel powerless at first, I was at a loss.\n",
    "\n",
    "Later, I asked myself to select only a few lines of information from twitter-archive-enhanced.csv in Excel. I hope to find some problems from it, and I found a literal problem. I recorded it like:\n",
    "- doggo, floofer, pupper, puppo content is an adjective, can be changed to 0, 1, in addition to hiding a field, you can add these 4 fields to know if multiple adjectives have different results\n",
    "- retweeted_status_id, retweeted_status_user_id, and retweeted_status_timestamp. The speculation is to forward the tweet id, which can be checked after the archive table and tweet json data are merged.\n",
    "\n",
    "Then I used Excel to open tweet_json.txt, but I encountered another problem. Because Excel can't translate the json format smoothly, I have to go back and use code to work. When I open it and it looks like an explosion, I originally wanted to use twitter-archive-enhanced.csv Observing the concept to work, but not very smoothly. But still try to list some items:\n",
    "- in_reply_to_screen_name, in_reply_to_status_id, in_reply_to_status_id_str, in_reply_to_user_id, in_reply_to_user_id_str are the same group. When this message is a reply tweet, the related message will be recorded. Then we only consider the original tweet data, so the data is deleted in response to the tweet.\n",
    "- ...\n",
    "\n",
    "For the first time, please see [git commit \"65d42e\"](https://github.com/wrayz/hw-wrangle-and-analyze-data/blob/65d42edbb8a0a9a01d64c88ca9da3570eebcf50c/wrangle_report.ipynb), the content can be written in Chinese Not suitable for English reading, but I want to express one thing from this commit. In the previous Access homework, I was ** guessing the definition of the field based on my own understanding of the content**, this way when doing clean work I found out that this is the wrong way**. The reason is when I question the basis of my speculation? If there is no basis for clean, can such information be used for analysis? The results of these self-answers are **NO**!\n",
    "\n",
    "To do this, I went to Google to find the twitter api document and quickly found a document [Tweet Objects](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object.html) It is the details of tweet_json.txt, which means that my Access assignments are coming back, but I have more confidence in this time to have more confidence to evaluate the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to document, I expect to re-accessing tweet_json.txt, then twitter-archive-enhanced.csv, and finally image-predictions.tsv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the proje proposal, I found that I ignored some of the contents of the object field, and this time I also corrected it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think back to my assessment process afterwards\n",
    "1. Understand the field definition\n",
    "2. Find missing data\n",
    "3. Check according to rules of tidy data\n",
    "4. Do it again after cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing a clean job, the original plan is to clean up one by one according to the Accessing column, but I noticed\n",
    "1. Accessing matters must be summarized again\n",
    "2. During cleaning, I still need to re-do the Accessing step and become cleaning-accessing-cleaning when necessary. The purpose of this is just to make sure that I am doing this without much error.\n",
    "\n",
    "Below I will list the following items according to different information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tweet json table\n",
    "\n",
    "- Remove unwanted fields, contributors, coordinates, geo, possible_sensitive, possible_sensitive_appealable, place, display_text_range, entities, favorited, retweeted, truncated, user\n",
    "- Remove extended_entities missing data\n",
    "- Delete in_reply_to_screen_name, in_reply_to_status_id, in_reply_to_status_id_str, in_reply_to_user_id, in_reply_to_user_id_str data with value\n",
    "- Delete data with is_quote_status True\n",
    "- Delete retweeted_status valued data\n",
    "- Check the rest of the data and remove the fields without data\n",
    "- Remove the is_quote_status, extended_entities field\n",
    "- Corrected id_str type as str datatype\n",
    "- Corrected lang type as category datatype\n",
    "- trim source content don't html info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter archive enhanced table\n",
    "- Merge Twitter archive enhanced table and Tweet json table.\n",
    "- Use info to check the information after the merge. If there is no data in the field, you can remove it directly.\n",
    "- Incorrect values in rating numerators\n",
    "- Calculate rating_numerator / rating_denominator and put it in a new field rating_ratio\n",
    "- Dog stages should be a single column rather than four\n",
    "- Correct dog name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image prediction table\n",
    "- Delete data with no variety (a total of 324)\n",
    "- Find the dog breed according to the priority of the forecast results and put it in the new open field.\n",
    "- merge with Twitter archive enhanced table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally I finally got a clean piece of information that I can use for analysis!\n",
    "\n",
    "I saved it to the twitter_archive_master.csv file and saved it to SqlLite. For me, this step is very easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1663 entries, 0 to 1662\n",
      "Data columns (total 15 columns):\n",
      "tweet_id          1663 non-null int64\n",
      "text              1663 non-null object\n",
      "expanded_urls     1663 non-null object\n",
      "name              1663 non-null object\n",
      "doggo             1663 non-null object\n",
      "floofer           1663 non-null object\n",
      "pupper            1663 non-null object\n",
      "puppo             1663 non-null object\n",
      "favorite_count    1663 non-null int64\n",
      "lang              1663 non-null object\n",
      "retweet_count     1663 non-null int64\n",
      "source            1663 non-null object\n",
      "created_at        1663 non-null object\n",
      "rating_ratio      1663 non-null float64\n",
      "breed             1663 non-null object\n",
      "dtypes: float64(1), int64(3), object(11)\n",
      "memory usage: 195.0+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('twitter_archive_master.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'wrangle_report.ipynb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
